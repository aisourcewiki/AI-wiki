<!--
title: Model training
description: 
published: true
date: 2023-04-16T17:10:17.436Z
tags: 
editor: ckeditor
dateCreated: 2023-04-08T19:53:51.877Z
-->

<h1>Model training</h1>
<h3>Overview</h3>
<p>The goal of a text-to-image model like Stable Diffusion is to produce “accurate” images based on a text prompt from the user.</p>
<p>Model training is the process of teaching a text-to-image model, so that it can produce the requested images. For text-to-image models, the model learns from training examples composed of images and their associated captions. By looking at many such examples, the model learns to associate words (e.g. cat) with their visual representations.</p>
<p>Models can be created either from scratch (e.g. Stable Diffusion), or can be developed from existing models, through a process known as “fine tuning”. Since the former is extremely expensive and impractical for most casual users, we'll cover fine tuning below.&nbsp;</p>
<h3>Fine tuning process</h3>
<p>The process for fine tuning a model is as follows, at a high level:</p>
<ul>
  <li><strong>1) Decide concept&nbsp;</strong>
    <ul>
      <li>Decide on a concept you want to teach your model</li>
    </ul>
  </li>
  <li><strong>2) Gather training data&nbsp;</strong>
    <ul>
      <li>Find a set of high quality training images relevant to the concept you have in mind</li>
      <li>(Optional) Describe the contents of the training images, either manually or using an automated system like BLIP or by hand</li>
    </ul>
  </li>
  <li><strong>3) Pick base model</strong>
    <ul>
      <li>Pick a base model that you want to train on top of. The more closely related the base model is to the concept you are trying to learn, the shorter your training process and the higher quality your results are likely to be. For instance, if you are training an anime character, consider starting from an anime model rather than a non-anime model like Stable Diffusion 1.5.</li>
    </ul>
  </li>
  <li><strong>4) Train model</strong>
    <ul>
      <li>Use an existing notebook or script to train your model - refer to the list of resources below.</li>
    </ul>
  </li>
  <li><strong>5) Model evaluation</strong>
    <ul>
      <li>Test the output of your model to see if it is good enough. If not, revise the work from earlier steps and then repeat this process until desired results are achieved.</li>
    </ul>
  </li>
</ul>
<p>As mentioned in the last step, the process above is meant to be iterative. It's fairly normal to test out multiple variations of captioning strategies or training parameter settings before arriving at good results.</p>
<h3>Captioning</h3>
<p>Captioning is the process by which you help the model understand what is in your images, and also help create associations between words and specific visual representations.&nbsp;</p>
<p>Ca</p>
<h3>Overtraining vs. undertraining</h3>
<p>When training models, there is often a general tradeoff between accuracy and diversity of results.</p>
<ul>
  <li>Accuracy - your model's ability to express the target “concept”</li>
  <li>Diversity - your model's ability to generate a variety of images</li>
</ul>
<p>If you do not train for enough steps, your model will not be able to reproduce the concept you have in mind, even if prompted. In the table below, an undertrained model intended to express “Scooby Doo” will not be able to do so.</p>
<p>However if you train for too many steps, while you may be able to reproduce the concept you have in mind, you will reduce the diversity of results across a variety of prompts. At this stage, prompts for “Scooby Doo” will often show him in exactly the same pose, while prompts for other subjects will suddenly start to show Scooby Doo as well.</p>
<p>A “balanced” model will be able to show Scooby Doo in a variety of situations and poses, while also not unnecessarily affecting other prompts not related to Scooby Doo (e.g. “Dalmatian” or “Cat”). As a simple test, if you generate 4 images using the same prompt (e.g. “Scooby Doo sitting”), and all the images look accurate but are different enough, then you likely have a balanced model.</p>
<p><strong>Comparison across all prompts</strong></p>
<figure class="table">
  <table>
    <tbody>
      <tr>
        <td><strong><u>Actual image v / Prompt &gt;&nbsp;</u></strong></td>
        <td><strong><u>“Scooby Doo”</u></strong></td>
        <td><strong><u>“Dalmatian”</u></strong></td>
        <td><strong><u>“Cat”</u></strong></td>
      </tr>
      <tr>
        <td><strong>Undertrained</strong></td>
        <td>(Random image)</td>
        <td>Dalmatian</td>
        <td>Cat</td>
      </tr>
      <tr>
        <td><strong>Balanced</strong></td>
        <td>Scooby Doo (diversity of poses)</td>
        <td>Dalmatian</td>
        <td>Cat</td>
      </tr>
      <tr>
        <td><strong>Overtrained</strong></td>
        <td>Scooby Doo (single pose repeated)</td>
        <td>Scooby Doo</td>
        <td>Scooby Doo</td>
      </tr>
    </tbody>
  </table>
</figure>
<p>&nbsp;</p>
<h3>Different ways to train</h3>
<p>Comparing class images - Native fine tuning vs. Dreambooth</p>
<p>Final file size - Loras vs. full models</p>
<p>See other pages for resources</p>
<h3>Important parameters</h3>
<p>Epochs</p>
<p>Batch size</p>
<h3>Notebooks &amp; code repositories</h3>
<figure class="table">
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Github</th>
        <th>Runpod</th>
        <th>Colab</th>
        <th>Other</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>EveryDream2trainer</td>
        <td><a href="https://github.com/victorchall/EveryDream2trainer">Link</a></td>
        <td><a href="https://github.com/victorchall/EveryDream2trainer/blob/main/doc/CLOUD_SETUP.md">Link</a></td>
        <td>-</td>
        <td><a href="https://github.com/victorchall/EveryDream2trainer/blob/main/doc/CLOUD_SETUP.md">Vast.ai</a></td>
      </tr>
      <tr>
        <td>Kohya Trainer</td>
        <td>-</td>
        <td>-</td>
        <td><a href="https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-trainer.ipynb">Link</a></td>
        <td>-</td>
      </tr>
    </tbody>
  </table>
</figure>
<h3>Additional resources</h3>
