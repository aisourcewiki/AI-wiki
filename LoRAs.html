<!--
title: LoRA
description: 
published: true
date: 2023-04-17T15:22:01.381Z
tags: 
editor: ckeditor
dateCreated: 2023-04-15T17:30:42.006Z
-->

<p><span class="text-small"><mark class="pen-red"><strong>Click the circle in the top right to edit!</strong></mark></span></p>
<h2>Intro</h2>
<p>LoRA stands for low-rank adaptation. It is a way to train a model that results in a smaller model size (~3-200MB &lt;&lt; 2GB), and can be used with either native fine tuning or Dreambooth.</p>
<p>Please refer to the information on training/inference for full models before proceeding further, since much of the info here is dependent on that prior knowledge.&nbsp;</p>
<h2>Getting started</h2>
<p>For resources on training models including code repositories and notebooks, visit the “Training Resources” page.</p>
<h2>Model training - guides</h2>
<ul>
  <li><a href="https://rentry.org/lora-training-science">https://rentry.org/lora-training-science</a></li>
  <li><a href="https://rentry.org/lora_train">https://rentry.org/lora_train</a></li>
</ul>
<h2>Inference</h2>
<h4>Auto1111</h4>
<ul>
  <li>Regular LoRAs are supported without extensions as of 1/23/23 (<a href="https://www.reddit.com/r/StableDiffusion/comments/10igjt2/auto1111_lora_native_support/">link</a>)</li>
  <li>Guide to using Lycoris (<a href="https://www.kombitz.com/2023/03/29/how-to-use-lycoris-locon-loha-models-with-automatic1111s-stable-diffusion-web-ui/">link</a>)</li>
</ul>
<h2>Terminology</h2>
<p><strong>LoRA </strong>- stands for low-rank adaptation, a mathematical technique to reduce the number of parameters that are trained. Results in a smaller model which is also faster to train. Technique can be applied to other types of models including LLMs, but has been especially popular for text-to-image models like Stable Diffusion (<a href="https://replicate.com/blog/lora-faster-fine-tuning-of-stable-diffusion">source</a>).</p>
<p><strong>Rank </strong>- rank is the size of the new “smaller” matrix used to store model weights. This is an adjustable parameter, which can be increased to store more details at the expense of larger file size (<a href="https://github.com/microsoft/LoRA">source</a>, for LLMs)</p>
<p><strong>LyCORIS </strong>-<strong> </strong>a project for making different algorithms for finetuning SD in parameter-efficient way, including LoRA (<a href="https://www.reddit.com/r/StableDiffusion/comments/11oz3z9/what_are_lycoris_lockon_models_and_what_is/">source, </a><a href="https://github.com/KohakuBlueleaf/LyCORIS">source</a>)</p>
<p><strong>LoHA </strong>-<strong> </strong>LoRA with Hadamard Product representation (LoHa). A specific type of LoRA which is more parameter efficient (<a href="https://github.com/KohakuBlueleaf/LyCORIS">source</a>, <a href="https://www.reddit.com/r/StableDiffusion/comments/11rbada/what_the_hell_is_a_loconloha_model/">source</a>)</p>
<p><strong>LoCon - </strong>LoRA using convolution (<a href="https://www.reddit.com/r/StableDiffusion/comments/11oz3z9/what_are_lycoris_lockon_models_and_what_is/">source</a>)</p>
<p><strong>Alpha&nbsp;</strong> - a LoRA training parameter which scales weight changes during training. Should be set relative to rank - “By setting alpha = dim you get alpha/dim = 1, which results in no scaling” (<a href="https://rentry.org/lora-training-science#network-alpha-20923">source</a>)</p>
