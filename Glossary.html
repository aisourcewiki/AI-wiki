<!--
title: Glossary
description: 
published: true
date: 2023-04-17T15:13:53.829Z
tags: 
editor: ckeditor
dateCreated: 2023-04-15T16:45:06.486Z
-->

<p style="text-align:center;"><span class="text-small"><mark class="pen-red"><strong>Click the circle in the top right to login and edit!</strong></mark></span></p>
<h1>Terms</h1>
<h2>General terminology</h2>
<p><strong>Text-to-image model</strong> - a machine learning model which produces images based on words provided. Stable Diffusion (Stability AI), Dall-E (OpenAI), and Midjourney are all examples of text-to-image models.&nbsp;</p>
<p><strong>Training </strong>- the process of teaching a machine learning model. For text-to-image models, the model learns from training examples composed of images and their associated captions. Training can either be “from scratch” (the model starts with no knowledge) or “fine tuning” (the model begins with some knowledge, but you teach it additional things).</p>
<p><strong>Training set</strong> - the data used to train the model. For text-to-image models, this must include images and sometimes also includes captions.</p>
<p><strong>Caption</strong> - the word(s) used to describe the contents of a training image. Also known as labels.&nbsp;</p>
<p><strong>Inference </strong>- the act of retrieving outputs from a trained model. For text-to-image models, this involves providing only a single prompt as input, and outputs one or more images based on the prompt.&nbsp;</p>
<p><strong>Prompt</strong> - the words provided by users to describe what images they want (e.g. “a cat dancing on the moon”)</p>
<p><strong>Fine tuning</strong> - the process of teaching an existing trained model additional information. Results in a new model file as output. For example if a trained model can already produce 100 different artistic styles, but doesn't know the style of Van Gogh, you can “fine tune” the model with images of Van Gogh's art, to produce a new model file that can replicate Van Gogh's style.&nbsp;</p>
<h2>Latent diffusion models - background</h2>
<p><strong>Diffusion model</strong> - a type of machine learning model which can produce a final result (e.g. an image) through a step-by-step process of removing noise, known as a diffusion process. In addition to image generation, they can also be used for video generation, audio synthesis, and reinforcement learning (<a href="https://huggingface.co/blog/stable_diffusion">source</a>, <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb">source</a>)&nbsp;</p>
<p><strong>Latent diffusion model </strong>- a type of diffusion model in which the diffusion process occurs in a lower dimensional (i.e. smaller) space called the latent space. Because the latent space is smaller, the image generation process is faster and takes less memory. Latent diffusion models have three main components - VAE, UNet, and text encoder. Notably Stable Diffusion is an implementation of a latent diffusion model (<a href="https://huggingface.co/blog/stable_diffusion">source</a>).</p>
<p><strong>Pixel space</strong> - where you are operating on the pixels of uncompressed images (<a href="https://huggingface.co/blog/stable_diffusion">source</a>)</p>
<p><strong>Latent space </strong>- the compressed space on which latent diffusion models operate. The VAE is responsible for transforming images from pixel space to the latent space (<a href="https://huggingface.co/blog/stable_diffusion">source</a>)</p>
<h2>Latent diffusion model - components</h2>
<p><strong>Text encoder</strong> - a component of latent diffusion models. Responsible for transforming the input prompt (e.g. "An astronaut riding a horse") into the latent space so it can be understood by the U-Net (<a href="https://jalammar.github.io/illustrated-stable-diffusion/">source</a>)</p>
<p><strong>UNet</strong> - a component of latent diffusion models. Helps to gradually remove noise from the initial noisy image to produce the final output (<a href="https://jalammar.github.io/illustrated-stable-diffusion/">source</a>)</p>
<p><strong>VAE</strong> - stands for variational autoencoder, and is one of the components of latent diffusion models. Composed of two parts, an encoder and a decoder. While the encoder responsible for transforming images from pixel space to latent space, the decoder transforms the latent representation back into an image. VAEs speed up the both the training and inference of latent diffusion models by allowing both to occur in latent space. Note that during inference, only the decoder is used (<a href="https://jalammar.github.io/illustrated-stable-diffusion/">source</a>, <a href="https://arxiv.org/pdf/2112.10752.pdf">source</a>)</p>
<h2>Model training - key concepts</h2>
<p><strong>Undertraining</strong> - when your model has not adequately learned the concept, because it has not been trained for enough steps. If you are trying to teach your model what Scooby Doo looks like, but it cannot produce Scooby Doo regardless of the prompt, the model may be undertrained.</p>
<p><strong>Overtraining</strong> - the opposite of under-training. Occurs when your model has been trained for too many steps - while it has learned the concept, it's learned the concept “too” well so it can't produce anything else, even if prompted to do so. For example, if your model always produces Scooby Doo even when prompted for a german shepherd or a cat, your model may be overtrained.</p>
<p><strong>Dreambooth</strong> - a method to fine tune models. Requires images for both the “instance” (the concept to be trained), as well as the “class” (the category to compare the instance against). For example if the instance is Scooby Doo, then the class can be “dog”. At a high level, Dreambooth tries to teach the model a new concept ("instance") while not harming the model's ability to reproduce images from the given class.&nbsp;</p>
<p><strong>Native fine tuning</strong> - fine tuning a model without Dreambooth.</p>
<p><strong>LoRA</strong> - a training method which produces a smaller file size. See “LoRA” section for more details.&nbsp;</p>
<h2>Model training - detailed terms</h2>
<p><strong>Epochs</strong> - the cycle of examining each of the instance dataset images once (<a href="https://www.reddit.com/r/StableDiffusion/comments/y6gjjz/question_about_training_epochs/">source</a>)</p>
<p><strong>Batch size</strong> - the number of images processed at once during training (<a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/4814">source</a>, <a href="https://github.com/victorchall/EveryDream-trainer">source</a>). Increasing speeds up training at the cost of increased VRAM usage.</p>
<p><strong>Training steps </strong>-<strong> </strong>the # of steps taken in fine tuning the model. The number of displayed training steps is roughly <strong>(</strong># of images in training set) x (# of epochs) / (batch size). Each batch effectively counts as a single training step (confirm this).&nbsp;</p>
<p><strong>EMA </strong>- exponential moving average, a technique used to improve inference performance and reduce the risk of "last-iterations overfitting" by taking the average of weights over a final period. Should be used only for inference, resuming model training (and perhaps fine tuning) should occur on the final (not averaged) weights (<a href="https://www.reddit.com/r/MachineLearning/comments/ucflc2/d_understanding_the_use_of_ema_in_diffusion_models/">source</a>, <a href="https://www.reddit.com/r/StableDiffusion/comments/x5am4v/ema_model_vs_non_ema_differences/">source</a>, <a href="https://huggingface.co/stabilityai/stable-diffusion-2-1/discussions/22">source)</a></p>
<p><strong>Class images </strong>- images representing the general category the instance images are a part of (and should be compared against).</p>
<p><strong>Regularization images</strong> - another name for the class images in Dreambooth training</p>
<h2>Inference</h2>
<p><strong>CFG </strong>- stands for classifier free guidance. Forces the generation to better match the prompt potentially at the cost of image quality or diversity (<a href="https://huggingface.co/blog/stable_diffusion">source</a>)&nbsp;</p>
<p><strong>Sampler</strong> - the method used to estimate noise at each step in the denoising process, which gradually produces a final image from an initial noisy image. There is often a trade-off between speed and accuracy of denoising. (<a href="https://stable-diffusion-art.com/samplers/#What_is_Sampling">source</a>)</p>
<h2>Other</h2>
<p><strong>Aesthetic gradient</strong> - TBD</p>
<p><strong>Hypernetwork</strong> - TBD</p>
<p>&nbsp;</p>
